SET 'auto.offset.reset' = 'earliest';

-- create events
CREATE STREAM "event_user_add" (
    "standard" STRING,
    "version" STRING,
    "event" STRING,
    "data" STRUCT <"id" STRING, "name" STRING, "avatar" STRING, "created_at" BIGINT, "updated_at" BIGINT>,
    "emit_info" STRUCT <"receipt_id" STRING, "block_timestamp" BIGINT, "block_height" BIGINT, "shard_id" BIGINT, "contract_account_id" STRING>
)
WITH (
    kafka_topic = 'near-events.sigilnet.user_add',
    value_format = 'json',
    partitions = 1
);

CREATE STREAM "event_user_update" (
    "standard" STRING,
    "version" STRING,
    "event" STRING,
    "data" STRUCT <"id" STRING, "name" STRING, "avatar" STRING, "created_at" BIGINT, "updated_at" BIGINT>,
    "emit_info" STRUCT <"receipt_id" STRING, "block_timestamp" BIGINT, "block_height" BIGINT, "shard_id" BIGINT, "contract_account_id" STRING>
)
WITH (
    kafka_topic = 'near-events.sigilnet.user_update',
    value_format = 'json',
    partitions = 1
);

-- create users
CREATE STREAM "user_add"
  WITH (kafka_topic='sigilnet.user_add', partitions=1, value_format='avro')
  AS SELECT
    "data"->"id" AS "id",
    "data"->"name" AS "name",
    "data"->"avatar" AS "avatar",
    "data"->"created_at" AS "created_at",
    "data"->"updated_at" AS "updated_at"
  FROM
    "event_user_add";

CREATE STREAM "user_update"
  WITH (kafka_topic='sigilnet.user_update', partitions=1, value_format='avro')
  AS SELECT
    "data"->"id" AS "id",
    "data"->"name" AS "name",
    "data"->"avatar" AS "avatar",
    "data"->"created_at" AS "created_at",
    "data"->"updated_at" AS "updated_at"
  FROM
    "event_user_update";

CREATE STREAM "users" ("id" STRING, "name" STRING, "avatar" STRING, "created_at" BIGINT, "updated_at" BIGINT)
  WITH (kafka_topic='sigilnet.users', partitions=1, value_format='avro');

INSERT INTO "users" SELECT * FROM "user_add";
INSERT INTO "users" SELECT * FROM "user_update";

-- create sink
CREATE SINK CONNECTOR "_users_pg" WITH (
    'connector.class'                         = 'io.confluent.connect.jdbc.JdbcSinkConnector',
    'connection.url'                          = 'jdbc:postgresql://postgres:5432/sigilnet',
    'connection.user'                         = 'sigilnet',
    'connection.password'                     = 'sigilnet',
    'topics'                                  = 'sigilnet.users',
    'key.converter'                           = 'org.apache.kafka.connect.storage.StringConverter',
    'key.converter.schema.registry.url'       = 'http://schema-registry:8081',
    'value.converter'                         = 'io.confluent.connect.avro.AvroConverter',
    'value.converter.schema.registry.url'     = 'http://schema-registry:8081',
    'auto.create'                             = 'true',
    'insert.mode'                             = 'upsert',
    'pk.mode'                                 = 'record_value',
    'pk.fields'                               = 'id'
);

CREATE SINK CONNECTOR "users" WITH (
    'connector.class'                         = 'com.mongodb.kafka.connect.MongoSinkConnector',
    'connection.uri'                          = 'mongodb://sigilnet:sigilnet@mongo:27017',
    'database'                                = 'sigilnet',
    'collection'                              = 'users',
    'tasks.max'                               = '1',
    'topics'                                  = 'sigilnet.users',
    'key.converter'                           = 'org.apache.kafka.connect.storage.StringConverter',
    'key.converter.schema.registry.url'       = 'http://schema-registry:8081',
    'value.converter'                         = 'io.confluent.connect.avro.AvroConverter',
    'value.converter.schema.registry.url'     = 'http://schema-registry:8081',
    'document.id.strategy' = 'com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy',
    'document.id.strategy.partial.value.projection.type' = 'AllowList',
    'document.id.strategy.partial.value.projection.list' = 'id'
);

-- testing
SHOW TOPICS;
SHOW STREAMS;
SHOW QUERIES;

DESCRIBE "users" EXTENDED;

SELECT * FROM "users" EMIT CHANGES;

TERMINATE <QUERYID>;
DROP STREAM <STREAM>;

-- TODO
2. postgres startup -> create schema
